{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4869222",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-17T13:31:09.281709Z",
     "start_time": "2024-09-17T13:31:06.154735Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import networkx as nx\n",
    "\n",
    "from utils import projection_order1, mask_from_order\n",
    "from data_generator import data_generator,generate_DAG, generator_matrix\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.set_default_dtype(torch.float64)\n",
    "import opt\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a688d92d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-17T13:31:09.887055Z",
     "start_time": "2024-09-17T13:31:09.301874Z"
    }
   },
   "outputs": [],
   "source": [
    "seed = 0\n",
    "torch.random.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "B_scale = 1.0 \n",
    "B_ranges = ((B_scale * -1.0, B_scale * -0.05), (B_scale * 0.05, B_scale * 1.0))\n",
    "n_test = 10000\n",
    "d = 100\n",
    "degree = 4\n",
    "graph_type = 'ER'\n",
    "noise_type = 'gaussian_ev'\n",
    "DAG = generate_DAG(d, graph_type, degree, B_ranges, hd = False, seed=seed)\n",
    "assert nx.is_directed_acyclic_graph(nx.DiGraph(DAG.B))\n",
    "B = torch.tensor(DAG.B).to(device)\n",
    "GM = generator_matrix(B)\n",
    "X_test = data_generator(generator_matrix=GM,bs=n_test)\n",
    "testing = False\n",
    "verbose = True\n",
    "def full_loss_(X, B):\n",
    "    return (0.5 / X.size()[0]) * torch.square(X - X @ B).sum()\n",
    "true_order = projection_order1(B)\n",
    "optimal_test_loss = full_loss_(X_test, B)\n",
    "main_mask = torch.eye(d).to(device) == 0\n",
    "D0 = torch.zeros(d,d).to(device) * main_mask\n",
    "D1 = torch.zeros(d,d).to(device) * main_mask\n",
    "true_distance = (D0-B).norm()\n",
    "distance = torch.ceil(true_distance)*2\n",
    "starting_loss = full_loss_(X_test, D0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c521bddde68bece9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-17T13:31:10.033248Z",
     "start_time": "2024-09-17T13:31:10.006158Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def full_loss_(X, B):\n",
    "    return (0.5 / X.size()[0]) * torch.square(X - X @ B).sum()\n",
    "true_order = projection_order1(B)\n",
    "optimal_test_loss = full_loss_(X_test, B)\n",
    "main_mask = torch.eye(d).to(device) == 0\n",
    "D0 = torch.zeros(d,d).to(device) * main_mask\n",
    "D1 = torch.zeros(d,d).to(device) * main_mask\n",
    "true_distance = (D0-B).norm()\n",
    "distance = torch.ceil(true_distance)*2\n",
    "starting_loss = full_loss_(X_test, D0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ccc93e5aba10186b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-17T13:31:24.707966Z",
     "start_time": "2024-09-17T13:31:24.693559Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "main_mask = torch.eye(d).to(device) == 0\n",
    "D0 = torch.zeros(d,d).to(device) * main_mask\n",
    "D1 = torch.zeros(d,d).to(device) * main_mask\n",
    "true_distance = (D0-B).norm()\n",
    "distance = torch.ceil(true_distance)*2\n",
    "starting_loss = full_loss_(X_test, D0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "431b39721b24a972",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-17T13:31:25.546317Z",
     "start_time": "2024-09-17T13:31:25.521999Z"
    }
   },
   "outputs": [],
   "source": [
    "starting_loss = full_loss_(X_test, D0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3bd157f6d1495d71",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-17T13:31:25.993982Z",
     "start_time": "2024-09-17T13:31:25.984994Z"
    }
   },
   "outputs": [],
   "source": [
    "#weight names\n",
    "additive='directional weights'\n",
    "noimportance='no importance'\n",
    "\n",
    "@torch.no_grad()\n",
    "def update_importance(elem1, elem2, weight1=1, weight2=1):\n",
    "    if weight1==0:\n",
    "        return elem2, 1\n",
    "    return (elem1 * weight1 + elem2 * weight2)/(weight1+weight2), weight1+weight2\n",
    "\n",
    "@torch.no_grad()\n",
    "def full_loss(D):\n",
    "    return (0.5 / X_test.size()[0]) * torch.square(X_test - X_test @ D).sum()\n",
    "\n",
    "def loss(D, bs, mask, strategy=noimportance):\n",
    "    x  = data_generator(GM, bs=bs)\n",
    "    columns_batch = torch.zeros(d)\n",
    "    if strategy!=noimportance:\n",
    "        with torch.no_grad():\n",
    "            for x_sample in x:\n",
    "                columns_batch += x_sample**2\n",
    "    return 0.5/bs  * torch.square(x - x @ (D * mask)).sum(), columns_batch/bs #+ 0.001* masked_D.norm(p=1) + 0.000 * masked_D.norm(p=2) ** 2\n",
    "\n",
    "def quadratic_optimization(D, optimizer, num_iter, mask, loss=loss, full_loss=full_loss, bs=1, log_iter=100, testing=testing, strategy='normal'):\n",
    "    losses_inside = []\n",
    "    columns_output = torch.zeros_like(D)\n",
    "    with torch.no_grad():\n",
    "        D *= mask\n",
    "    for i in range(num_iter):\n",
    "        if i % log_iter == 0 and testing:\n",
    "            losses_inside.append(full_loss(D).item())\n",
    "        def closure():\n",
    "            optimizer.zero_grad()\n",
    "            loss_closure, columns_new = loss(D=D,bs=bs,mask=mask, strategy=strategy)\n",
    "            nonlocal columns_output\n",
    "            columns_output,_ = update_importance(columns_output, columns_new, weight1=i)\n",
    "            return loss_closure\n",
    "        optimizer.step(closure)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        D *= mask\n",
    "    return D, losses_inside, columns_output\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2510c7e252fc59",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-17T13:32:32.656083Z",
     "start_time": "2024-09-17T13:31:26.839383Z"
    }
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "\n",
    "best_loss = 1e20\n",
    "best_D = D1.clone()\n",
    "best_order = None\n",
    "\n",
    "epochs = 100\n",
    "num_iter_outer = 2000\n",
    "num_iter_inner = 2000\n",
    "bs = 1\n",
    "testing = False\n",
    "verbose = False\n",
    "\n",
    "strategy=noimportance\n",
    "\n",
    "# ==========================================================================================\n",
    "# RANDOM ORDER\n",
    "# \n",
    "results_random_order=[]\n",
    "for rseed in range(1000):\n",
    "    torch.random.manual_seed(rseed)\n",
    "    np.random.seed(rseed)\n",
    "    print('Random order {}/1000'.format(rseed+1))\n",
    "    importance_columns, number_of_samples = torch.zeros(d,d).to(device), 0\n",
    "    D = D0.clone().detach().requires_grad_()\n",
    "    optimizer = opt.UniversalSGD([D], D=distance)\n",
    "    main_losses = [full_loss(D).item()]\n",
    "\n",
    "    order=torch.randperm(d)    #####\n",
    "    mask=mask_from_order(order, main_mask) ####\n",
    "    for j in tqdm(range(epochs)):\n",
    "        D, loss, columns_new = quadratic_optimization(D,optimizer,num_iter_inner, mask=mask, testing=testing, strategy=strategy)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            loss2 = full_loss(D * mask).item()\n",
    "            if verbose:\n",
    "                print('after', loss2)\n",
    "            main_losses.append(loss2)\n",
    "            if loss2 < best_loss:\n",
    "                best_loss = loss2 + 0.\n",
    "                best_D = D.detach().clone()\n",
    "                best_order = order + 0\n",
    "    results_random_order.append({'strategy':strategy, 'seed':seed, 'main_losses':main_losses, 'D':D})\n",
    "    \n",
    "# ==========================================================================================\n",
    "# CORRECT ORDER\n",
    "# \n",
    "rseed=0\n",
    "torch.random.manual_seed(rseed)\n",
    "np.random.seed(rseed)\n",
    "print('Correct order')\n",
    "importance_columns, number_of_samples = torch.zeros(d,d).to(device), 0\n",
    "D = D0.clone().detach().requires_grad_()\n",
    "optimizer = opt.UniversalSGD([D], D=distance)\n",
    "main_losses = [full_loss(D).item()]\n",
    "\n",
    "order=true_order    #####\n",
    "mask=mask_from_order(order, main_mask) ####\n",
    "for j in tqdm(range(epochs)):\n",
    "    D, loss, columns_new = quadratic_optimization(D,optimizer,num_iter_inner, mask=mask, testing=testing, strategy=strategy)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        loss2 = full_loss(D * mask).item()\n",
    "        if verbose:\n",
    "            print('after', loss2)\n",
    "        main_losses.append(loss2)\n",
    "        if loss2 < best_loss:\n",
    "            best_loss = loss2 + 0.\n",
    "            best_D = D.detach().clone()\n",
    "            best_order = order + 0\n",
    "results_true_order={'strategy':strategy, 'seed':seed, 'main_losses':main_losses, 'D':D}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9beeb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "for result in [results_random_order[0]]:\n",
    "        plt.semilogy(torch.tensor(result['main_losses'])-optimal_test_loss, label='Random order, seeds 0-99')\n",
    "plt.semilogy(torch.tensor(results_true_order['main_losses'])-optimal_test_loss, label='Correct order', marker='*', markevery=20)\n",
    "for result in results_random_order[1:100]:\n",
    "        plt.semilogy(torch.tensor(result['main_losses'])-optimal_test_loss)\n",
    "\n",
    "plt.ylabel(r' $f(x_k)-f(\\bar{x})$')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.savefig('plot_random_order.pdf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51253d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pylab as pl\n",
    "\n",
    "plt.hist([torch.tensor(result['main_losses'][-1])-optimal_test_loss for result in results_random_order], label='Random order')\n",
    "pl.hist([results_true_order['main_losses'][-1]-optimal_test_loss], bins=np.logspace(-2,1, 50), label='Correct order')\n",
    "pl.gca().set_xscale(\"log\")\n",
    "plt.ylabel('Count')\n",
    "plt.xlabel('Suboptimality after 100 SGD epochs')\n",
    "plt.title('Vertices: {}'.format(d))\n",
    "plt.legend()\n",
    "plt.savefig('histogram_random_order.pdf')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a9a54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "\n",
    "# with open('random_orders.pickle', 'wb') as handle:\n",
    "#     pickle.dump((results_random_order, results_true_order), handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "# with open('random_orders.pickle', 'rb') as handle:\n",
    "#     results_random_order, results_true_order = pickle.load(handle)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
